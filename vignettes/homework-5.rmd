---
title: "Homework 5"
author: "Wenfeng Zhang"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Homework 5 vignette}
-->


## Question 1
In class we used the LASSO to predict hand-writting characters in the MNIST data set. Increase the out-of-sample prediction accuracy by extracting predictive features from the images.

```{r}
library(keras)
library(glmnet)
library(doMC)

registerDoMC()

# The data, shuffled and split between train and test sets
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

x_train <- array_reshape(x_train, c(60000, 28^2))
x_test <- array_reshape(x_test, c(10000, 28^2))
y_train <- factor(y_train)
y_test <- factor(y_test)


# Original model
set.seed(23331)
s <- sample(seq_along(y_train), 1000)
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")
preds <- predict(fit$glmnet.fit, x_train[s,], s = fit$lambda.min, 
                 type = "class")
# table(as.vector(preds), y_train[s])

# Out of sample error
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")
t <- table(as.vector(preds), y_test)
a <- sum(diag(t)) / sum(t)
a

# Extracting predictive features
# We can decrease lambda to include more predictive features
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min*0.8,
                 type = "class")
t <- table(as.vector(preds), y_test)
sum(diag(t)) / sum(t)
```
Out of sample accuracy of the original model using lambda.min is 84.51%, when we decrease lambda to incorporate more predictive features, the out of sample accuracy increased to 84.6%



## Question 2
CASL Number 4 in Exercises 8.11  

Adjust the kernel size, and any other parameters you think are useful, in the convolutional neural network for EMNIST in Section 8.10.4. Can you improve on the classification rate?  
```{r}
load("emnist.Rdata")
# train set
X_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim = c(nrow(X_train), 28, 28))
for (i in 1:nrow(X_train)) {
  x_train[i,,] <- matrix(X_train[i,], nrow=28, ncol=28)
}
y_train <- as.vector(emnist$dataset[[1]][[2]])-1
y_train <- to_categorical(y_train, num_classes = 26)
# test set
X_test <- emnist$dataset[[2]][[1]]/255
x_test <- array(dim = c(nrow(X_test), 28, 28))
for (i in 1:nrow(X_test)) {
  x_test[i,,] <- matrix(X_test[i,], nrow=28, ncol=28)
}
y_test <- as.vector(emnist$dataset[[2]][[2]])-1
y_test <- to_categorical(y_test)

x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))

# original model in textbook
model <- keras_model_sequential()
model %>%
  layer_conv_2d(filters = 32, kernel_size = c(2, 2), input_shape = c(28, 28, 1), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2, 2), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 32, kernel_size = c(2, 2), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(2, 2), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy', optimizer = optimizer_rmsprop(), metrics = c('accuracy'))
# model %>% fit(x_train, y_train, epochs = 10, validation_data = list(x_test, y_test), batch_size = 128)
# save the model and load the model to save time
# model %>% save_model_hdf5("original_model.h5")
model <- load_model_hdf5("original_model.h5")
model %>% keras::evaluate(x_test, y_test, verbose = 0)

# modified model
model1 <- keras_model_sequential()
model1 %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), input_shape = c(28, 28, 1), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")

model1 %>% compile(loss = 'categorical_crossentropy', optimizer = optimizer_rmsprop(), metrics = c('accuracy'))
# model1 %>% fit(x_train, y_train, epochs = 10, validation_data = list(x_test, y_test), batch_size = 128)
# model1 %>% save_model_hdf5("modified_model.h5")
model1 <- load_model_hdf5("modified_model.h5")
model1 %>% keras::evaluate(x_test, y_test, verbose = 0)
```
Two models were built. The first is the model from CASL 8.10.4, and the accuracy on test set is 88.26% (not 90.39% shown in textbook, the reason could be the dataset I used is different from the dataset in textbook). After modifying kernal size (from (2,2) to (3,3)) and dropout rate (from 0.5 to 0.4), the accuracy increased to 90.95%. To save time, the model is saved and reloaded instead of building model everytime we knit. 


## Question 3
CASL Number 8 in Exercises 8.11

Write a function that uses mean absolute deviation as a loss function, instead of mean squared error. Test the use of this function with a simulation containing several outliers. How well do neural networks and SGD perform when using robust techniques?  

Create list of weights to describe a dense neural network.
```{r}
# Args:
#     sizes: A vector giving the size of each layer, including
#
#
# Returns:
#     A list containing initialized weights and biases.
casl_nn_make_weights <- function(sizes){
  L <- length(sizes) - 1L
  weights <- vector("list", L)
  for (j in seq_len(L)){
    w <- matrix(rnorm(sizes[j] * sizes[j + 1L]),
                ncol = sizes[j], nrow = sizes[j + 1L])
    weights[[j]] <- list(w=w,b=rnorm(sizes[j + 1L]))
  }
  weights 
}
```
Next, we need to define the ReLU function for the forward pass:
```{r}
# Apply a rectified linear unit (ReLU) to a vector/matrix.
#
# Args:
# v: A numeric vector or matrix.
#
# Returns:
# The original input with negative values truncated to zero.
casl_util_ReLU <-
function(v)
{
v[v < 0] <- 0
v
}
```
And the derivative of the ReLU function for the backwards pass:
```{r}
# Apply derivative of the rectified linear unit (ReLU).
#
# Args:
# v: A numeric vector or matrix.
#
# Returns:
# Sets positive values to 1 and negative values to zero.
casl_util_ReLU_p <-
function(v)
{
p <- v * 0
p[v > 0] <- 1
p
}
```
We also need to differentiate the loss function for backpropagation. Here we
use mean squared error (multiplied by 0.5).
```{r}
# Derivative of the mean squared error (MSE) function.
#
# Args:
# y: A numeric vector of responses.
# a: A numeric vector of predicted responses.
#
# Returns:
# Returned current derivative the MSE function.
casl_util_mse_p <-
function(y, a)
{
(a - y)
}
```



```{r}
# Apply forward propagation to a set of NN weights and biases.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     weights: A list created by casl_nn_make_weights.
#     sigma: The activation function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
casl_nn_forward_prop <-
function(x, weights, sigma)
{
L <- length(weights)
z <- vector("list", L)
a <- vector("list", L)
for (j in seq_len(L))
{
  a_j1 <- if(j == 1) x else a[[j - 1L]]
z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
    a[[j]] <- if (j != L) sigma(z[[j]]) else z[[j]]
  }
  list(z=z, a=a)
}
```

```{r}
# Apply backward propagation algorithm.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     y: A numeric vector representing one row of the response.
#     weights: A list created by casl_nn_make_weights.
#     f_obj: Output of the function casl_nn_forward_prop.
#     sigma_p: Derivative of the activation function.
#     f_p: Derivative of the loss function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
casl_nn_backward_prop <-
  function(x, y, weights, f_obj, sigma_p, f_p)
{
  z <- f_obj$z; a <- f_obj$a
  L <- length(weights)
grad_z <- vector("list", L)
grad_w <- vector("list", L)
for (j in rev(seq_len(L)))
{
if (j == L) {
grad_z[[j]] <- f_p(y, a[[j]])
} else {
  grad_z[[j]] <- (t(weights[[j + 1]]$w) %*%
                  grad_z[[j + 1]]) * sigma_p(z[[j]])
}
a_j1 <- if(j == 1) x else a[[j - 1L]]
grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
  }
  list(grad_z=grad_z, grad_w=grad_w)
}
```

```{r}
# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A numeric data matrix.
#     y: A numeric vector of responses.
#     sizes: A numeric vector giving the sizes of layers in
#            the neural network.
#     epochs: Integer number of epochs to computer.
#     eta: Positive numeric learning rate.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.
casl_nn_sgd <-
function(X, y, sizes, epochs, eta, weights=NULL)
{
  if (is.null(weights))
{
  weights <- casl_nn_make_weights(sizes)
}
for (epoch in seq_len(epochs))
{
  for (i in seq_len(nrow(X)))
  {
    f_obj <- casl_nn_forward_prop(X[i,], weights,
casl_util_ReLU)
b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
f_obj, casl_util_ReLU_p,
casl_util_mse_p)
for (j in seq_along(b_obj))
{
  weights[[j]]$b <- weights[[j]]$b -
                      eta * b_obj$grad_z[[j]]
  weights[[j]]$w <- weights[[j]]$w -
eta * b_obj$grad_w[[j]]
}
} }
weights }
```


```{r}
# Predict values from a training neural network.
#
# Args:
#     weights: List of weights describing the neural network.
#     X_test: A numeric data matrix for the predictions.
#
# Returns:
#     A matrix of predicted values.
casl_nn_predict <-
function(weights, X_test)
{p <- length(weights[[length(weights)]]$b)
y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
for (i in seq_len(nrow(X_test)))
{
  a <- casl_nn_forward_prop(X_test[i,], weights,
                            casl_util_ReLU)$a
  y_hat[i, ] <- a[[length(a)]]
}
y_hat }
```

Using MAD as loss function
```{r}
mad_p <- function(y, a){
  (a-y)/abs(a-y)
}

casl_nn_mad_sgd <-
function(X, y, sizes, epochs, eta, weights=NULL)
{
  if (is.null(weights))
{
  weights <- casl_nn_make_weights(sizes)
}
for (epoch in seq_len(epochs))
{
  for (i in seq_len(nrow(X)))
  {
    f_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
    b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights, f_obj, casl_util_ReLU_p, mad_p)
for (j in seq_along(b_obj))
{
  weights[[j]]$b <- weights[[j]]$b - eta * b_obj$grad_z[[j]]
  weights[[j]]$w <- weights[[j]]$w - eta * b_obj$grad_w[[j]]
}
} }
weights }
```


```{r}
set.seed(2333)
X <- matrix(runif(1000, min=-1, max=1), ncol=1)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
outid <- sample(seq_along(y), 50)
y[outid,] = sample(c(10,50,100,-25),1)
```
If we use mse as loss function
```{r}
weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01)
y_pred <- casl_nn_predict(weights, X)
sum((y-y_pred)^2/1000)
sum((y[-outid,]-y_pred[-outid,])^2/1000)
```
If we use mad as loss function
```{r}
weights <- casl_nn_mad_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01)
y_pred <- casl_nn_predict(weights, X)
sum((y-y_pred)^2/1000)
sum((y[-outid,]-y_pred[-outid,])^2/1000)

```

Adding 50 outliers to dataset, the MSE(defined as $(predicted-actual)^2/n$) of model using MAD as loss function is smaller than that of the model using MSE as loss function, MAD outperforms MSE in terms of robustness